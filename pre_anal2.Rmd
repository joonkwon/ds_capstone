---
title: "Text Minining"
author: "Joohyun Kwon"
date: "May 19, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r cars}
library(tm)
library(ggplot2)
```

To make data cleaning easier, we sample texts from documents - 1% for blog and twitter, 5% for news.

```{r cache=TRUE}
blogs = readLines('./final/en_US/en_US.blogs.txt', encoding = 'UTF-8')
set.seed(465)
blogs <- sample(blogs, round(length(blogs)/100))
writeLines(blogs, con="./final/en_US/en_US.blogs.sample.txt", sep="\n")
#below will concatenate all lines (multi length character vector) to one line character vector
#blogs <- paste(blogs, sep="", collapse=" ")
docs_blog <- Corpus(VectorSource(blogs))
```

```{r}
# testing generic remove functions
# remove punctuation
blog = tm_map(docs_blog, removePunctuation)
blog = tm_map(blog, removeNumbers)

#checking output change the line numbers
writeLines(as.character((blog[1:10])))
```

```{r}
#function that remove everything but space and alphabet
removeNumSpecial <- function(x) { gsub("[^a-zA-Z[:space:]]*", " ", x)}
docs_blog <- tm_map(docs_blog, content_transformer(removeNumSpecial))
docs_blog <- tm_map(docs_blog, gsub, "[^[:alpha:][:space:]]*", " ")
```

```{r}

# remove \"
#dblogs = tm_map(docs_blog, gsub, pattern="\\\"", replacement = " ")
# remove some special character and puctuation
#dblogs = tm_map(dblogs, gsub, pattern ="[\\?\\.\\*!]", replacement = " ")
# remove *, %, :, ;, (, ), =, #, ~, /, - and numbers
#dblogs = tm_map(dblogs, gsub, pattern ="[\\*%0-9:;\\)\\(=#~/\\-\\?\\.,!]", replacement = " ")
```
```{r}
#inspect
inspect(dblogs[100:110])
```


```{r}
# remove whitespace
dblogs = tm_map(dblogs, stripWhitespace)
```
```{r}
#find word that used at the beginning of Sentences
function startWord(sentence) {
    
}


```

```{r cache=TRUE}
news = readLines('./final/en_US/en_US.news.txt', encoding = 'UTF-8')
set.seed(376)
news = sample(news, round(length(news)/20))
docs_news = Corpus(VectorSource(news))                          
                          
```

```{r cache=TRUE}
twitter = readLines('./final/en_US/en_US.twitter.txt', encoding = 'UTF-8')
set.seed(380)
twitter = sample(twitter, round(length(twitter)/100))
docs_twitter = Corpus(VectorSource(twitter))                          
```

```{r cache=TRUE}
#docs = c(docs_blog, docs_news, docs_twitter)
```


Define a content transformer that will substitute matching pattern to single space
```{r}
toSpace = content_transformer(function(x, pattern) { return (gsub(pattern, ' ', x))})
```

replace ':' and '-' with space using above toSpace content transformer
```{r cache=TRUE}
#docs = tm_map(docs, toSpace, ':')
#docs = tm_map(docs, toSpace, '-')
#docs = tm_map(docs, toSpace, '?')
#docs = tm_map(docs, toSpace, '.')
#docs = tm_map(docs, toSpace, ';')
#docs = tm_map(docs, toSpace, ')')
#docs = tm_map(docs, toSpace, '(')
```

not removing punctuaton. will manually remove one by one. or find a way to remove in bulk. I would like to keep "'" in "don't", "I've", etc.
```{r cache=TRUE}
#docs = tm_map(docs, removePunctuation)
docs = tm_map(docs, removeNumbers)
#docs = tm_map(docs, content_transformer(tolower))
```

```{r cache=TRUE}

#docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, stripWhitespace)
```

load library for stemming
```{r}
#library(SnowballC)
```
```{r cache=TRUE}
#docs = tm_map(docs, stemDocument)
```
```{r}
dtm = DocumentTermMatrix(docs)
```



