---
title: "Text Minining"
author: "Joohyun Kwon"
date: "May 19, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r cars}
library(tm)
library(ggplot2)
```

To make data cleaning easier, we sample texts from documents - 1% for blog and twitter, 5% for news.

```{r cache=TRUE}
blog = readLines('./final/en_US/en_US.blogs.txt', encoding = 'UTF-8')
set.seed(465)
blog <- sample(blog, round(length(blog)/100))
docs_blog <- Corpus(VectorSource(blog))
```

```{r}
# remove \"
dblogs = tm_map(docs_blog, gsub, pattern="\\\"", replacement = " ")
# remove some special character and puctuation
#dblogs = tm_map(dblogs, gsub, pattern ="[\\?\\.\\*!]", replacement = " ")
# remove *, %, :, ;, (, ), =, #, ~, /, - and numbers
dblogs = tm_map(dblogs, gsub, pattern ="[\\*%0-9:;\\)\\(=#~/\\-]", replacement = " ")

# remove whitespace
dblogs = tm_map(dblogs, stripWhitespace)
```
```{r}
#find word that used at the beginning of Sentences
function startWord(sentence) {
    
}


```

```{r cache=TRUE}
news = readLines('./final/en_US/en_US.news.txt', encoding = 'UTF-8')
set.seed(376)
news = sample(news, round(length(news)/20))
docs_news = Corpus(VectorSource(news))                          
                          
```

```{r cache=TRUE}
twitter = readLines('./final/en_US/en_US.twitter.txt', encoding = 'UTF-8')
set.seed(380)
twitter = sample(twitter, round(length(twitter)/100))
docs_twitter = Corpus(VectorSource(twitter))                          
```

```{r cache=TRUE}
#docs = c(docs_blog, docs_news, docs_twitter)
```


Define a content transformer that will substitute matching pattern to single space
```{r}
toSpace = content_transformer(function(x, pattern) { return (gsub(pattern, ' ', x))})
```

replace ':' and '-' with space using above toSpace content transformer
```{r cache=TRUE}
#docs = tm_map(docs, toSpace, ':')
#docs = tm_map(docs, toSpace, '-')
#docs = tm_map(docs, toSpace, '?')
#docs = tm_map(docs, toSpace, '.')
#docs = tm_map(docs, toSpace, ';')
#docs = tm_map(docs, toSpace, ')')
#docs = tm_map(docs, toSpace, '(')
```

not removing punctuaton. will manually remove one by one. or find a way to remove in bulk. I would like to keep "'" in "don't", "I've", etc.
```{r cache=TRUE}
#docs = tm_map(docs, removePunctuation)
docs = tm_map(docs, removeNumbers)
#docs = tm_map(docs, content_transformer(tolower))
```

```{r cache=TRUE}

#docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, stripWhitespace)
```

load library for stemming
```{r}
#library(SnowballC)
```
```{r cache=TRUE}
#docs = tm_map(docs, stemDocument)
```
```{r}
dtm = DocumentTermMatrix(docs)
```



