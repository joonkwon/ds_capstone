---
title: "sw_pred_c"
author: "Joohyun Kwon"
date: "June 30, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r echo=FALSE, message=FALSE, warning=FALSE}
options(java.parameters = "-Xmx18000m")
library(tm)
library(ggplot2)
library(knitr)
library(qdap)
library(RWeka)
library(wordcloud)
library(stringi)
```

```{r}
SAMPLE_MODE = TRUE
```

Sample Training/Testing Data to 90%, 10%

```{r}
file_dir = "./final/en_US"

file_dir_train = "./final/en_US/training"
file_dir_test = "./final/en_US/testing"
wk_dir_train = "./wk_dir_train"
wk_dir_sample = "./wk_dir_sample"

ifelse(!dir.exists(file_dir_train), dir.create(file_dir_train), FALSE)
ifelse(!dir.exists(file_dir_test), dir.create(file_dir_test), FALSE)

ifelse(!dir.exists(wk_dir_train), dir.create(wk_dir_train), FALSE)
ifelse(!dir.exists(wk_dir_sample), dir.create(wk_dir_sample), FALSE)

if (!file.exists(file.path(file_dir_train, "en_US.twitter.txt")) |
    !file.exists(file.path(file_dir_train, "en_US.news.txt")) |
    !file.exists(file.path(file_dir_train, "en_US.blogs.txt")) ) 
{
    corpus = Corpus(DirSource(file_dir, encoding='UTF-8' ))
    for (i in 1:length(corpus)) {
        document_filename = corpus[[i]]$meta$id
        document_length = length(corpus[[i]]$content)
        all_idx = 1:document_length
        set.seed(1005)
        train_idx =sample(all_idx, document_length * 0.9)
        train = corpus[[i]]$content[train_idx]
        test = corpus[[i]]$content[-train_idx]
        train = train[!is.na(train)]
        test = test[!is.na(test)]
        write(train, file.path(file_dir_train, document_filename))
        write(test, file.path(file_dir_test, document_filename))
        rm(test,train, train_idx, all_idx)
    }    
    rm(corpus)
}
```
Create a small set of data to efficiently write code
```{r}
file_dir_sample = './final/en_US/sample'
ifelse(!dir.exists(file_dir_sample), dir.create(file_dir_sample), FALSE)
if (!file.exists(file.path(file_dir_sample, "en_US.twitter.txt")) |
    !file.exists(file.path(file_dir_sample, "en_US.news.txt")) |
    !file.exists(file.path(file_dir_sample, "en_US.blogs.txt")) ) 
{
    corpus = Corpus(DirSource(file_dir_train, encoding='UTF-8' ),
                    readerControl=list(language="en"))
    
    for (i in 1:length(corpus)) {
        document_filename = corpus[[i]]$meta$id
        document_length = length(corpus[[i]]$content)
        all_idx = 1:document_length
        set.seed(2005)
        sample_idx =sample(all_idx, document_length * 0.01)
        sample_docs = corpus[[i]]$content[sample_idx]
        sample_docs = sample_docs[!is.na(sample_docs)]
        write(sample_docs, file.path(file_dir_sample, document_filename))
        rm(sample_docs, sample_idx, all_idx)
    }  
    rm(corpus)
}
```

```{r}
file_dir = file_dir_train
wk_dir = wk_dir_train
if (SAMPLE_MODE) { 
  file_dir = './final/en_US/sample' 
  wk_dir = wk_dir_sample
}
```


```{r}
qdap_clean = function(x) {
    # x is character vector
    #replace utf single quote to ascii single quote (for apostrophe)
    #x = gsub("\u2019", "\u0027", x)
    x = gsub("<92>", "\u0027", x)
    x = replace_abbreviation(x)
    #mark End of Sentence
    x = gsub("\\. |\\.$", " EEEOSSS ", x)
    x = gsub("\\! |\\!$", " EEEOSSS ", x)
    x = gsub("\\? |\\?$", " EEEOSSS ", x)
    #remove all the non-alphabets and single quote. We did this before removing contraction 
    #because some non ascii character throw errors
    x = gsub("[^a-zA-Z\u0027]", " ", x)
    #x = replace_contraction(x)

    # remove single quote completely
    #x = gsub("\u0027", " ", x)
    x = tolower(x)
    
    print("gdap_clean one loop completed")
    
    return(x)
}
```

```{r}
toSpace = content_transformer(function(x, pattern) { return (gsub(pattern, ' ', x))})

tm_clean = function(x){
    #x is a corpus
    x <- tm_map(x, stripWhitespace)
    x = tm_map(x, toSpace, ' [^iIaA] ')
    x = tm_map(x, toSpace, '^[^iIaA] ')
    x = tm_map(x, toSpace, ' [^iIaA]$')
    x <- tm_map(x, stripWhitespace)
#    x <- tm_map(x, tolower) #tolower throw error
    
    print("tm_clean completed")
    
    return(x)
}
```

```{r cache = TRUE}
Sys.time()
docs = Corpus(DirSource(file_dir, encoding='UTF-8' ))
Sys.time()
```

###Words and Lines Count of Each Document

```{r echo=FALSE}
ln = length(docs)
filenames = vector(mode="character",ln)
wordsCount = vector(mode="numeric",ln)
linesCount = vector(mode="numeric",ln)

for (i in 1:ln){
    filenames[i] = docs[[i]]$meta$id
}
for (i in 1:ln){
    linesCount[i] = length(docs[[i]]$content)
}

for (i in 1:ln){
    
    wordsCount[i] = stri_stats_latex(docs[[i]]$content)["Words"]
}

simpleStat = data.frame(filenames, linesCount, wordsCount)
kable(simpleStat, col.names = c("Filename", "Lines Count", "Words Count"))
```



##Cleaning Text and Creating Corpus
```{r text_clean, cache = TRUE}
Sys.time()
for (i in 1:length(docs)) {
    docs[[i]]$content = qdap_clean(docs[[i]]$content)
}

docs = tm_clean(docs)
Sys.time()
```
```{r}
saveRDS(docs, file.path(wk_dir, "corpus_docs.RDS"))
```

```{r helper_fucntion}
tokenizer_bigram = function(x) {
    NGramTokenizer(x, Weka_control(min =2, max=2, delimiters=' \r\n\t'))
}

tokenizer_trigram = function(x) {
    NGramTokenizer(x, Weka_control(min =3, max=3, delimiters=' \r\n\t'))
}

# tokenizer_quadgram = function(x) {
#     NGramTokenizer(x, Weka_control(min =4, max=4))
# }


removeEOS = function(x) {
    EOS_idx = grepl("eeeosss", colnames(x))
    x = x[,!EOS_idx]
    return(x)
}


```

```{r}
Sys.time()
dtm_uni = DocumentTermMatrix(docs)
Sys.time()
```

##20 Most Frequent Words
```{r dtm_uni, cache = TRUE}
freq_uni = vector()
freq_uni = readRDS(file.path(wk_dir,"freq_uni.RDS"))

if (length(freq_uni) == 0) {
  dtm_uni = DocumentTermMatrix(docs)
  dtm_uni_m = as.matrix(dtm_uni)
  rm(dtm_uni) # delete to save memory
  dtm_uni_m = removeEOS(dtm_uni_m)
  freq_uni = colSums(dtm_uni_m)
  freq_uni = sort(freq_uni, decreasing = TRUE)
  saveRDS(freq_uni, file.path(wk_dir,"freq_uni.RDS"))
}

```

```{r}
barplot(freq_uni[1:20], col='tan', las=2)
```

##Wordcloud

```{r}
wordcloud(names(freq_uni), freq_uni, max.words = 100, colors = brewer.pal(8,"Dark2"),
          random.color = TRUE)
```

##20 Most Frequent 2-grams
```{r dtm_bi, cache=TRUE}
Sys.time()
freq_bi = vector()
freq_bi = readRDS(file.path(wk_dir,"freq_bi.RDS"))
if (length(freq_bi) == 0) {
  dtm_bi = DocumentTermMatrix(docs, control = list(tokenize=tokenizer_bigram))
  dtm_bi_m = as.matrix(dtm_bi)
  rm(dtm_bi)
  dtm_bi_m = removeEOS(dtm_bi_m)
  freq_bi = colSums(dtm_bi_m)
  rm(dtm_bi_m)
  freq_bi = sort(freq_bi, decreasing = TRUE)
  saveRDS(freq_bi, file.path(wk_dir,"freq_bi.RDS"))
}

freq_bi[1:20]
Sys.time()
```


##20 Most Frequent 3-grams

```{r dtm_tri, cache=TRUE}
freq_tri = vector()
freq_tri = readRDS(file.path(wk_dir,"freq_tri.RDS"))
if (length(freq_tri) == 0) {
  dtm_tri = DocumentTermMatrix(docs, control = list(tokenize=tokenizer_trigram))
  dtm_tri_m = as.matrix(dtm_tri)
  rm(dtm_tri)
  dtm_tri_m = removeEOS(dtm_tri_m)
  freq_tri = colSums(dtm_tri_m)
  rm(dtm_tri_m)
  freq_tri = sort(freq_tri, decreasing = TRUE)
  saveRDS(freq_tri, file.path(wk_dir,"freq_tri.RDS"))
}

freq_tri[1:20]
```


