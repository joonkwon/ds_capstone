---
title: "Milestone Report - Predictive Keyboard Capstone Project"
author: "Joohyun Kwon"
date: "June 30, 2016"
output: html_document
---
#Synopsis
A smart keyboard like SwiftKey helps people to type better on mobile devices. we will apply data analysis and NLP (Natural Language Process) on data aquired from blogs, news and twitter in order to build a predictive model for typing. When a user type a few words, our program will predict the next word. This will help in building a smart keyboard.  

#Executive Summary
I present a basic EDA on the data. This will help understand the data. N-gram analysis is also conducted. Our model will be build using N-gram modeling and the roadmap for the model building is discussed too.

#Getting and Cleaning Data
The course provided the data. The data was downloaded from https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip. Besides English, there are other language data too - French, German and Russian. We will focus on English only. 
After download, the data were devided to training and testing data. I used only training data to build models and testign data will be used to test the efficacy of the models.
All the cleaning works has been done using a small sample to speed up the process. English has about 600MB data. Becuase of the size, any data processing will take long time. I sampled only 1% of training data to develop proper data cleaning process.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r echo=FALSE, message=FALSE, warning=FALSE}
options(java.parameters = "-Xmx18000m")
library(tm)
library(ggplot2)
library(knitr)
library(qdap)
library(RWeka)
library(wordcloud)
library(stringi)
```

```{r}
SAMPLE_MODE = FALSE
DATA_PROCESSED = TRUE
```

###Devision of Training and Testing Data

I devided the data to Training and Testing set (90% vs. 10%).

```{r}
file_dir = "./final/en_US"

file_dir_train = "./final/en_US/training"
file_dir_test = "./final/en_US/testing"
wk_dir_train = "./wk_dir_train"
wk_dir_sample = "./wk_dir_sample"

ifelse(!dir.exists(file_dir_train), dir.create(file_dir_train), FALSE)
ifelse(!dir.exists(file_dir_test), dir.create(file_dir_test), FALSE)

ifelse(!dir.exists(wk_dir_train), dir.create(wk_dir_train), FALSE)
ifelse(!dir.exists(wk_dir_sample), dir.create(wk_dir_sample), FALSE)

if (!file.exists(file.path(file_dir_train, "en_US.twitter.txt")) |
    !file.exists(file.path(file_dir_train, "en_US.news.txt")) |
    !file.exists(file.path(file_dir_train, "en_US.blogs.txt")) ) 
{
    corpus = Corpus(DirSource(file_dir, encoding='UTF-8' ))
    for (i in 1:length(corpus)) {
        document_filename = corpus[[i]]$meta$id
        document_length = length(corpus[[i]]$content)
        all_idx = 1:document_length
        set.seed(1005)
        train_idx =sample(all_idx, document_length * 0.9)
        train = corpus[[i]]$content[train_idx]
        test = corpus[[i]]$content[-train_idx]
        train = train[!is.na(train)]
        test = test[!is.na(test)]
        write(train, file.path(file_dir_train, document_filename))
        write(test, file.path(file_dir_test, document_filename))
        rm(test,train, train_idx, all_idx)
    }    
    rm(corpus)
}
```
### Sampling Data
I create a small set (1%) of sampled data to efficiently run code while developing.
```{r}
file_dir_sample = './final/en_US/sample'
ifelse(!dir.exists(file_dir_sample), dir.create(file_dir_sample), FALSE)
if (!file.exists(file.path(file_dir_sample, "en_US.twitter.txt")) |
    !file.exists(file.path(file_dir_sample, "en_US.news.txt")) |
    !file.exists(file.path(file_dir_sample, "en_US.blogs.txt")) ) 
{
    corpus = Corpus(DirSource(file_dir_train, encoding='UTF-8' ),
                    readerControl=list(language="en"))
    
    for (i in 1:length(corpus)) {
        document_filename = corpus[[i]]$meta$id
        document_length = length(corpus[[i]]$content)
        all_idx = 1:document_length
        set.seed(2005)
        sample_idx =sample(all_idx, document_length * 0.01)
        sample_docs = corpus[[i]]$content[sample_idx]
        sample_docs = sample_docs[!is.na(sample_docs)]
        write(sample_docs, file.path(file_dir_sample, document_filename))
        rm(sample_docs, sample_idx, all_idx)
    }  
    rm(corpus)
}
```

```{r}
file_dir = file_dir_train
wk_dir = wk_dir_train
if (SAMPLE_MODE) { 
  file_dir = './final/en_US/sample' 
  wk_dir = wk_dir_sample
}
```

###Data Cleaning
I treat a word with apostrophe as one word. So contraction is not devided to two words. Also apostrophe in possessive form is kept too. All other special characters and numbers are removed. The end of sentence are marked with an end-of-sentence marker (EEEOSSS). This will later be removed from n-gram.

```{r}
qdap_clean = function(x) {
    # x is character vector
    #replace utf single quote to ascii single quote (for apostrophe)
    #x = gsub("\u2019", "\u0027", x)
    x = gsub("<92>", "\u0027", x)
    x = replace_abbreviation(x)
    #mark End of Sentence
    x = gsub("\\. |\\.$", " EEEOSSS ", x)
    x = gsub("\\! |\\!$", " EEEOSSS ", x)
    x = gsub("\\? |\\?$", " EEEOSSS ", x)
    #remove all the non-alphabets and single quote. We did this before removing contraction 
    #because some non ascii character throw errors
    x = gsub("[^a-zA-Z\u0027]", " ", x)
    #x = replace_contraction(x)

    # remove single quote completely
    #x = gsub("\u0027", " ", x)
    x = tolower(x)
    
    print("gdap_clean one loop completed")
    
    return(x)
}
```

```{r}
toSpace = content_transformer(function(x, pattern) { return (gsub(pattern, ' ', x))})

tm_clean = function(x){
    #x is a corpus
    x <- tm_map(x, stripWhitespace)
    x = tm_map(x, toSpace, ' [^iIaA] ')
    x = tm_map(x, toSpace, '^[^iIaA] ')
    x = tm_map(x, toSpace, ' [^iIaA]$')
    x <- tm_map(x, stripWhitespace)
#    x <- tm_map(x, tolower) #tolower throw error
    
    print("tm_clean completed")
    
    return(x)
}
```

```{r cache = TRUE}
docs = Corpus(DirSource(file_dir, encoding='UTF-8' ))
```

###Words and Lines Count of Each Document
There are three documents on the data (blogs, news and twitter). Below are the Line and words counts of each document.

```{r echo=FALSE}
ln = length(docs)
filenames = vector(mode="character",ln)
wordsCount = vector(mode="numeric",ln)
linesCount = vector(mode="numeric",ln)

for (i in 1:ln){
    filenames[i] = docs[[i]]$meta$id
}
for (i in 1:ln){
    linesCount[i] = length(docs[[i]]$content)
}

for (i in 1:ln){
    
    wordsCount[i] = stri_stats_latex(docs[[i]]$content)["Words"]
}

simpleStat = data.frame(filenames, linesCount, wordsCount)
kable(simpleStat, col.names = c("Filename", "Lines Count", "Words Count"))
```



##Cleaning Text and Creating Corpus
```{r text_clean, cache = TRUE}
if (DATA_PROCESSED) {

    docs = readRDS(file.path(wk_dir, "corpus_docs.RDS"))

} else {

  for (i in 1:length(docs)) {
      docs[[i]]$content = qdap_clean(docs[[i]]$content)
  }

  docs = tm_clean(docs)
  saveRDS(docs, file.path(wk_dir, "corpus_docs.RDS"))
}
```

```{r helper_fucntion}
tokenizer_bigram = function(x) {
    NGramTokenizer(x, Weka_control(min =2, max=2, delimiters=' \r\n\t'))
}

tokenizer_trigram = function(x) {
    NGramTokenizer(x, Weka_control(min =3, max=3, delimiters=' \r\n\t'))
}

removeEOS = function(x) {
    EOS_idx = grepl("eeeosss", colnames(x))
    x = x[,!EOS_idx]
    return(x)
}

```



#20 Most Frequent Words
```{r dtm_uni, cache = TRUE}
freq_uni = vector()
freq_uni = readRDS(file.path(wk_dir,"freq_uni.RDS"))

if ( length(freq_uni) == 0 | !DATA_PROCESSED ) {
  dtm_uni = DocumentTermMatrix(docs)
  dtm_uni_m = as.matrix(dtm_uni)
  rm(dtm_uni) # delete to save memory
  dtm_uni_m = removeEOS(dtm_uni_m)
  freq_uni = colSums(dtm_uni_m)
  freq_uni = sort(freq_uni, decreasing = TRUE)
  saveRDS(freq_uni, file.path(wk_dir,"freq_uni.RDS"))
}
```

```{r}
barplot(freq_uni[1:20], col='tan', las=2)
```

#Wordcloud

```{r}
wordcloud(names(freq_uni), freq_uni, max.words = 100, colors = brewer.pal(8,"Dark2"),
          random.color = TRUE)
```

#20 Most Frequent 2-grams
```{r dtm_bi, cache=TRUE}
freq_bi = vector()
freq_bi = readRDS(file.path(wk_dir,"freq_bi.RDS"))
if (length(freq_bi) == 0 | !DATA_PROCESSED ) {
  dtm_bi = DocumentTermMatrix(docs, control = list(tokenize=tokenizer_bigram))
  dtm_bi_m = as.matrix(dtm_bi)
  rm(dtm_bi)
  dtm_bi_m = removeEOS(dtm_bi_m)
  freq_bi = colSums(dtm_bi_m)
  rm(dtm_bi_m)
  freq_bi = sort(freq_bi, decreasing = TRUE)
  saveRDS(freq_bi, file.path(wk_dir,"freq_bi.RDS"))
}

barplot(freq_bi[1:20], col='tan', las=2)
```


##20 Most Frequent 3-grams

```{r dtm_tri, cache=TRUE}
freq_tri = vector()
freq_tri = readRDS(file.path(wk_dir,"freq_tri.RDS"))
if (length(freq_tri) == 0 | !DATA_PROCESSED ) {
  dtm_tri = DocumentTermMatrix(docs, control = list(tokenize=tokenizer_trigram))
  dtm_tri_m = as.matrix(dtm_tri)
  rm(dtm_tri)
  dtm_tri_m = removeEOS(dtm_tri_m)
  freq_tri = colSums(dtm_tri_m)
  rm(dtm_tri_m)
  freq_tri = sort(freq_tri, decreasing = TRUE)
  saveRDS(freq_tri, file.path(wk_dir,"freq_tri.RDS"))
}

barplot(freq_tri[1:20], col='tan', las=2)

```

###Fequency Distribution of 1-gram, 2-gram and 3-gram
```{r}
#par(mfrow=c(1,3))
plot(log10(freq_uni))
plot(log10(freq_bi))
plot(log10(freq_tri))
```

