pred1 = predict(modelFit, testing)
pred1
testing[,grep("^IL", names(testing))]
head(testing[,grep("^IL", names(testing))])
pred1 = predict(modelFit, testing[,grep("^IL", names(testing))])
pred1
summary(pred1)
modelFit
modelFit2
modelFitpc
modelFitpca
rm(list=ls())
data("Wage")
inTrain = createDataPartition(y=Wage$wage,p=0.7, list=FALSE)
traing = Wage[inTrain,]; testing = Wage[-inTrain,]
head(Wage)
str(Wage)
dummies = dummyVars(wage ~ jobclass, data=training)
training= traing
dummies = dummyVars(wage ~ jobclass, data=training)
head(predict(dummies,newdata=training))
dim(predict(dummies,newdata=training))
dim(training)
head(training)
?Wage
rm(traing)
nearZeroVar(training,saveMetrics=TRUE)
?nearZeroVar
nearZeroVar(training)
library(splines)
baBasis <- bs(training$age, df=3)
lm1 <- lm(wage ~ baBasis, data = training)
plot(training$age, training$wage, pch=19, cex=0.5)
points(training$age, predict(lm1,newdata=training), col="red", pch=190, cex=.5)
points(training$age, predict(lm1,newdata=testing), col="red", pch=190, cex=.5)
plot(lm)
plot(lm1)
plot(training$age, training$wage, pch=19, cex=0.5)
abline(lm1)
head(baBasis)
0.4308138^2
0.4308138^3
head(training)
dim(baBasis)
dim(training)
bst = predict(baBasis, age=testing$age)
lm2 = lm(wage ~ bst, data=testing)
dim(bst)
libray(ISLR)
library(spam)
data(spam)
head(spam)
prComp = prcomp(log10(spam[,-58] +1))
head(prComp$rotation)
plot(prComp)
names(prComp)
prComp$x
dim(prComp$x)
dim(spam)
head(prComp$rotation)
dim(prComp$rotation)
inTrain = createDataPartition(spam$type, p=0.75, list=FALSE)
training = spam[inTrain,]
testing = spam[-inTrain,]
preProc = preProcess(log10(training[,-58] + 1), method="pca", pcaComp=2)
trainPC = predict(preProc, log10(training[,-58] +1 ))
modelFit = train(training$type ~ . method="glm", data=trainPC)
names(trainPC)
dim(trainPC)
class(trainPC)
modelFit = train(training$type ~ ., method="glm", data=trainPC)
summary(modelFit)
testPC = predict(preProc, log10(testing[,-58]+1))
confusionMatrix(testing$type, predict(modelFit,testPC))
fit1 = train(training$type ~ ., method="glm", preProcess="pca", data=training)
confusionMatrix(testing$type, predict(fit1, data=testing))
confusionMatrix(testing$type, predict(fit1, testing))
rm(list=ls())
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
train = training[,grep("^IL", names(training))]
dim(training)
dim(train)
train = training[,c(grep("^IL", names(training)),"diagnosis")]
names(training)
train = training[,c(grep("^IL", names(training)),diagnosis)]
dim(train)
dim(training)
train = training[,c(grep("^IL", names(training)),"diagnosis")]
train = training[,c(grep("^IL", names(training)),diagnosis)]
grep("^IL", names(training))
train = training[,c(grep("^IL", names(training)),1)]
dim(train)
head(train)
pre = preProcess(train[,-1], method="pca", thresh=0.8)
names(pre)
summary(pre)
pcatrain = predict(pre,train[,-1])
pcatrain
names(pcatrain)
fit = train(train$diagnosis ~ . method="glm", data=pcatrain)
fit = train(train$diagnosis ~ ., method="glm", data=pcatrain)
summary(fit)
test = testing[,c(1,grep("^IL", names(testing)))]
head(test)
head(train)
train = training[,c(1,grep("^IL", names(testing)))]
head(train)
pre = preProcess(train[,-1], method="pca",thresh=0.8)
trainpc = predict(pre, train[,-1])
testpc = predict(pre, test[,-1])
fit = train(train$diagnosis ~ ., method="glm", data=trainpc)
head(trainpc)
summary(fit)
confusionMatrix(test$diagnosis, predict(fit, testpc))
fit.lm = train(train$diagnosis ~ ., method="glm", data=train)
summary(fit.lm)
confusionMatrix(test$diagnosis, predict(fit.lm, test))
rm(list=ls)
rm(list=ls())
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
head(mixtures)
rm(list=ls())
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
head(mixtures)
head(row.names(training))
plot(row.names(training), training$CompressiveStrength)
plot(row.names(training), training$CompressiveStrength, colour=Cement)
plot(row.names(training), training$CompressiveStrength, colour=training$Cement)
plot(row.names(training), training$CompressiveStrength, color=training$Cement)
plot(row.names(training), training$CompressiveStrength, colors=training$Cement)
warnings()
plot(row.names(training), training$CompressiveStrength, colours=training$Cement)
plot(row.names(training), training$CompressiveStrength, col=training$Cement)
plot(row.names(training), training$CompressiveStrength, colour=training$Cement)
qplot(row.names(training), training$CompressiveStrength, colour=training$Cement)
library(Hmisc)
cements = cut2(training$Cement)
head(cements)
qplot(row.names(training), training$CompressiveStrength, colour=cements)
names(training)
fly = cut2(training$FlyAsh)
qplot(row.names(training), training$CompressiveStrength, colour=fly)
age = cut2(training$Age)
qplot(row.names(training), training$CompressiveStrength, colour=age)
plot(training$age, training$CompressiveStrength)
dim(training)
plot(training$age, training$CompressiveStrength)
length(training$age)
plot(training$Age, training$CompressiveStrength)
qplot(row.names(training), training$CompressiveStrength, colour=training$Age)
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
train = training[,grep("^IL", names(training))]
head(train)
pre = preProcess(train, method="pca", thresh=0.9)
pre
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
training$CompressiveStrength
training$SuperPlasticizer
head(training)
hist(training$Superplasticizer)
hist(log(training$Superplasticizer) + 1)
hist(log(training$Superplasticizer+1))
rm(list=ls())
data("iris")
inTrain = createDataPartition(y=iris$Species, p=.7, list=FALSE)
training = iris[inTrain,]
testing = isris[-inTrain,]
testing = iris[-inTrain,]
modfFit = train(Species ~ ., method="rpart", data=training)
print(modfFit$finalModel)
rattle::fancyRpartPlot(modfFit$finalModel)
install.packages(rattle)
install.packages("rattle")
rattle::fancyRpartPlot(modfFit$finalModel)
predict(modFit, newdata=testing)
predict(modfFit, newdata=testing)
rm(list=ls())
install.packages("ElemStatLearn")
library(ElemStatLearn)
data("ozone",package="ElemStatLearn")
?order
head(ozone)
ozone = ozone[order(ozone$ozone),]
head(ozone)
order(ozone$ozone)
names(Computational Statistics)
names(order(ozone$ozone))
dim(ozone)
ll = matrix(NA, nrow=10, ncol=155)
?sample
ss = sample(1:dim(ozone)[1], replace=T)
ss
order(ss)
head(ozone)
ll
summary(ozone$ozone)
for ( i in 1:10) {
ss <- sample(1:dim(ozone)[1], replace=TRUE)TRUE
ozone0 <- ozone[ss,]
ozone0 <- ozone0[order(ozone0$ozone),]
loess0 <- loess(temperature ~ ozone, data = ozone0, span = 0.2)
ll[i,] <- predict(loess0, newdata = data.frame(ozone=1:155))
}
for ( i in 1:10) {
ss <- sample(1:dim(ozone)[1], replace=TRUE)
ozone0 <- ozone[ss,]
ozone0 <- ozone0[order(ozone0$ozone),]
loess0 <- loess(temperature ~ ozone, data = ozone0, span = 0.2)
ll[i,] <- predict(loess0, newdata = data.frame(ozone=1:155))
}
plot(ozone$oone, ozone$temperature)
plot(ozone$ozone, ozone$temperature)
plot(ozone$ozone, ozone$temperature, pch=19, cex=0.5)
lines(1:155, ll[1,], col="grey", lwd=2)
lines(1:155, ll[2,], col="grey", lwd=2)
lines(1:155, ll[3,], col="grey", lwd=2)
lines(1:155, ll[4,], col="grey", lwd=2)
lines(1:155, ll[5,], col="grey", lwd=2)
lines(1:155, ll[6,], col="grey", lwd=2)
lines(1:155, ll[7,], col="grey", lwd=2)
lines(1:155, ll[8,], col="grey", lwd=2)
lines(1:155, ll[9,], col="grey", lwd=2)
lines(1:155, ll[10,], col="grey", lwd=2)
?apply
lines(1:155, apply(ll,2, mean), col="led", lwd=2)
lines(1:155, apply(ll,2, mean), col="red", lwd=2)
rm(list=ls())
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
head(segmentationOriginal)
dim(segmentationOriginal)
inTrin = createDataPartition(segmentationOriginal$Case, p = 0.7, list = FALSE)
set.seed(125)
training = segmentationOriginal[inTrin,]
testing = segmentationOriginal[-inTrin,]
modelfit = train(Case ~ ., method="rpart", data=training)
pred1 = predict(modelfit, data.frame(c(TotalIntench2=23000,FiberWidthCh1=10,PerimStatusCh1=2)))
pred1 = predict(modelfit, as.data.frame(c(TotalIntench2=23000,FiberWidthCh1=10,PerimStatusCh1=2)))
as.data.frame(c(a=1,b=2,c=3))
as.data.frame(a=1,b=2,c=3)
as.data.frame(cbind(a=1,b=2,c=3)
)
data.frame(a=1,b=2,c=3)
pred1 = predict(modelfit, data.frame(cTotalIntench2=23000,FiberWidthCh1=10,PerimStatusCh1=2))
pred1 = predict(modelfit, data.frame(TotalIntench2=23000,FiberWidthCh1=10,PerimStatusCh1=2))
pred1 = predict(modelfit, data.frame(TotalIntenCh2=23000,FiberWidthCh1=10,PerimStatusCh1=2))
pred1 = predict(modelfit, data.frame(TotalIntenCh2=23000,FiberWidthCh1=10,PerimStatusCh1=2))
pred1 = predict(modelfit, newdata=data.frame(TotalIntenCh2=23000,FiberWidthCh1=10,PerimStatusCh1=2))
ata.frame(TotalIntenCh2=23000,FiberWidthCh1=10,PerimStatusCh1=2)
data.frame(TotalIntenCh2=23000,FiberWidthCh1=10,PerimStatusCh1=2)
suppressMessages(library(rattle))
library(rpart.plot)
modelfit$finalModel
fancyRpartPlot(modelfit$finalModel)
head(training)
dim(train)
dim(training)
dim(testing)
summary(segmentationOriginal$Case)
training = segmentationOriginal[segmentationOriginal$Case=="Train",]
dim(training)
testing = segmentationOriginal[segmentationOriginal$Case=="Test",]
dim(testing)
head(training)
modelFit = train(Class ~ . ,data=training, method="rpart")
modelFit$finalModel
fancyRpartPlot(modelFit$finalModel)
install.packages("pgmm")
library(pgmm)
head(olive)
data(olive)
head(olive)
tail(olive)
olive=olive[,-1]
dim(olive)
modelFit = train(Area ~ ., data=olive, method="ctree")
modelFit = train(Area ~ ., data=olive, method="rpart")
plot(modelFit$finlaModel, uniform=TRUE)
plot(modelFit$finalModel, uniform=TRUE)
fancyRpartPlot(modelFit$finalModel)
colMeans((olive))
t(colMeans((olive)))
?t
newdata = as.data.frame(t(colMeans((olive))))
pd = predict(modelFit, newdata)
pd
summary(olive$Area)
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
set.seed(13234)
head(train)
head(trainSA)
modelFit = train(chd ~ age + alcohol + obesity + tobacco + typea + ldl, data=trainSA, method="glm", family="binomial")
summary(trainSA$chd)
modelFit = train(as.factor(chd) ~ age + alcohol + obesity + tobacco + typea + ldl, data=trainSA, method="glm", family="binomial")
modelFit
modelFit$finalModel
fit1 = train(chd ~ age + alcohol + obesity + tobacco + typea + ldl, data=trainSA, method="glm", family="binomial")
fit1
fit1$finalModel
missClass = function(values,prediction) { sum((prediction > 0.5) *1 != values)/length(values)}
missClass(trainSA$chd,predict(fit1, trainSA))
missClass(testSA$chd,predict(fit1, testSA))
data(vowel.train)
data(vowel.test)
head(vowel.train)
vowel.train$y = as.factor(vowel.train$y)
head(vowel.train)
summary(vowel.train$y)
str(vowel.train)
vowel.test$y = as.factor(vowel.test$y)
str(vowel.test)
?train
tree = train(y ~ ., data=vowel.train, method="rf")
tree
varImp(tree)
set.seed(33833)
tree = train(y ~ ., data=vowel.train, method="rf")
varImp(tree)
set.seed(33833)
tree = train(y ~ ., data=vowel.train, method="rf", prox=TRUE)
varImp(tree)
?randomForest
rftree = randomForest(y ~ ., data=vowel.train)
varImp(rftree)
order(varImp(rftree))
setwd("~/git-hub/ds_capstone")
library(tm)
library(SnowballC)
textfile = './final/en_US/pg1001.txt'
if (!file.exists(textfile)) {
download.file("http://www.gutenberg.org/ebooks/100.txt.utf-8", destfile = textfile)
}
shakespeare = readLines(textfile)
shakespeare = shakespeare[-(1:173)]
shakespeare = shakespeare[-(124195:length(shakespeare))]
shakespeare = paste(shakespeare, collapse = " ")
shakespeare = strsplit(shakespeare, "<<[^>]*>>")[[1]]
dramatis.personae = grep("Dramatis Personae", shakespeare, ignore.case = TRUE)
shakespeare = shakespeare[-dramatis.personae]
docs = Corpus(VectorSource(shakespeare))
docs = tm_map(docs, tolower)
docs = tm_map(docs, removePunctuation)
docs = tm_map(docs, removeNumbers)
docs = tm_map(docs, removeWords, stopwords("en"))
docs = tm_map(docs, stemDocument)
docs = tm_map(docs, stripWhitespace)
dtm = DocumentTermMatrix(docs)
dtm = DocumentTermMatrix(docs)
summary(docs)
inspect(docs)
summary(docs)
rm(list=ls())
textfile = './final/en_US/pg1001.txt'
if (!file.exists(textfile)) {
download.file("http://www.gutenberg.org/ebooks/100.txt.utf-8", destfile = textfile)
}
shakespeare = readLines(textfile)
shakespeare[165:175]
shakespeare = shakespeare[-(1:173)]
shakespeare = shakespeare[-(124195:length(shakespeare))]
shakespeare = paste(shakespeare, collapse = " ")
shakespeare = strsplit(shakespeare, "<<[^>]*>>")[[1]]
dramatis.personae = grep("Dramatis Personae", shakespeare, ignore.case = TRUE)
shakespeare = shakespeare[-dramatis.personae]
docs = Corpus(VectorSource(shakespeare))
summary(docs)
docs = tm_map(docs, tolower)
docs = tm_map(docs, removePunctuation)
docs = tm_map(docs, removeNumbers)
docs = tm_map(docs, removeWords, stopwords("en"))
docs = tm_map(docs, stemDocument)
docs[1]
docs[[1]]
docs = tm_map(docs, stripWhitespace)
docs[[1]]
docs = tm_map(docs, stemDocument)
docs[[1]]
dtm = DocumentTermMatrix(docs)
dtm <- DocumentTermMatrix(docs)
rm(list=ls())
textfile = './final/en_US/pg1001.txt'
if (!file.exists(textfile)) {
download.file("http://www.gutenberg.org/ebooks/100.txt.utf-8", destfile = textfile)
}
shakespeare = readLines(textfile)
shakespeare = shakespeare[-(1:173)]
shakespeare = shakespeare[-(124195:length(shakespeare))]
shakespeare = paste(shakespeare, collapse = " ")
shakespeare = strsplit(shakespeare, "<<[^>]*>>")[[1]]
dramatis.personae = grep("Dramatis Personae", shakespeare, ignore.case = TRUE)
shakespeare = shakespeare[-dramatis.personae]
docs = Corpus(VectorSource(shakespeare))
docs = tm_map(docs, content_transformer(tolower))
docs = tm_map(docs, removePunctuation)
docs = tm_map(docs, removeNumbers)
docs = tm_map(docs, removeWords, stopwords("en"))
docs = tm_map(docs, stemDocument)
docs = tm_map(docs, stripWhitespace)
dtm = DocumentTermMatrix(docs)
summary(dtm)
dtm
inspect(dtm[1:10, 1:10])
findFreqTerms(dtm, 2000)
dim(dtm)
?removeSparseTerms
freq = colSums(as.matrix(dtm))
head(freq)
order = order(freeny, decreasing = TRUE)
order
ord = order(freq, decreasing = TRUE)
head(ord)
top10 = freq[ord[1:10]]
top10
top10 = freq[ord[1:100]]
top10
findAssocs(dtm, "blood", 0.8)
findAssocs(dtm, "blood", 0.5)
findAssocs(dtm, "kill", 0.5)
findAssocs(dtm, "thought", 0.5)
findAssocs(dtm, "thought", 0.8)
findAssocs(dtm, "thought", 0.7)
summary(docs)
bounds = list(global = c(3,182))))
dtmr = DocumentTermMatrix(docs, control = list(wordLength=c(3,20),
bounds = list(global = c(3,182))))
dim(dtmr)
dim(dtm)
fr = colSums(dtmr)
fr = colSums(as.matrix(dtmr))
head(fr)
order_fr = order(fr, decreasing = TRUE)
head(order_fr)
fr[order_fr[1:10]]
dtmr = DocumentTermMatrix(docs, control = list(wordLength=c(3,20),
bounds = list(global = c(3,181))))
dim(dtmr)
dim(docs)
summary(docs)
dtmr = DocumentTermMatrix(docs, control = list(wordLength=c(3,20),
bounds = list(global = c(3,180))))
dim(dtmr)
fr2 = colSums(as.matrix(dtmr))
order_fr2 = order(fr2, decreasing=TRUE)
fr2[order_fr2[1:10]]
top10
dtmr = DocumentTermMatrix(docs, control = list(wordLength=c(3,20),
bounds = list(global = c(3,160))))
fr2 = colSums(as.matrix(dtmr))
order_fr2 = order(fr2, decreasing=TRUE)
top10_2 = fr2[order_fr2[1:10]]
top10_2
top100_2 = fr2[order_fr2[1:100]]
top100_2
dtmr = DocumentTermMatrix(docs, control = list(wordLength=c(3,20),
bounds = list(global = c(3,182))))
fr2 = colSums(as.matrix(dtmr))
order_fr2 = order(fr2, decreasing=TRUE)
top100_2 = fr2[order_fr2[1:100]]
top100_2
dim(dtmr)
dim(dtm)
?paste0
